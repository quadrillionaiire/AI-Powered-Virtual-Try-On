{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Powered Virtual Try On\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import the Libraries\n",
    "Before we can use tools like MediaPipe to analyze images, we need to clean and organize the data. This step ensures that our data is in the correct format and ready for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings to keep output clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display progress bars\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load & Validate Data and Create Pairs Manually\n",
    "\n",
    "Sometimes, the file containing image pairs (train_pairs.txt) needs manual formatting. We manually create a dataframe where each row corresponds to a pair of an image and a piece of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image pairs (manually formatted from train_pairs.txt)\n",
    "image_cloth_pairs = [\n",
    "    (\"00577_00.jpg\", \"00024_00.jpg\"),\n",
    "    (\"00018_00.jpg\", \"04879_00.jpg\"),\n",
    "    (\"00025_00.jpg\", \"06990_00.jpg\"),\n",
    "    (\"00023_00.jpg\", \"04608_00.jpg\"),\n",
    "    (\"00009_00.jpg\", \"07274_00.jpg\"),\n",
    "    (\"00000_00.jpg\", \"08733_00.jpg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for pairs\n",
    "data = pd.DataFrame(image_cloth_pairs, columns=[\"image\", \"cloth\"])\n",
    "\n",
    "# Show the dataframe\n",
    "print(\"Manually created image-cloth pairs:\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Validate and Organize Files\n",
    "Before preprocessing, ensure that all the required files are in their respective folders. If any file is missing, this step will alert us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the raw data folder paths\n",
    "RAW_DATA_PATH = \"/data/raw/train\"\n",
    "IMAGE_FOLDER = os.path.join(RAW_DATA_PATH, \"image\")\n",
    "CLOTH_FOLDER = os.path.join(RAW_DATA_PATH, \"cloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check for missing files\n",
    "missing_files = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    image_path = os.path.join(IMAGE_FOLDER, row['image'])\n",
    "    cloth_path = os.path.join(CLOTH_FOLDER, row['cloth'])\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        missing_files.append(image_path)\n",
    "    if not os.path.exists(cloth_path):\n",
    "        missing_files.append(cloth_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"Missing files detected:\", missing_files)\n",
    "else:\n",
    "    print(\"All files are present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Images: Why Resize and Normalize?\n",
    "To make all images uniform in size for analysis and to scale pixel values between 0 and 1 (normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for processed data\n",
    "# PROCESSED_DATA_PATH = \"./data/processed/\"\n",
    "# os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "IMG_SIZE = (256, 192)  # Resize dimensions (width, height)\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path, output_path, img_size):\n",
    "    \"\"\"\n",
    "    Resize and normalize an image, then save it.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Resize the image\n",
    "    img_resized = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Normalize the image (scale pixel values between 0 and 1)\n",
    "    img_normalized = img_resized / 255.0\n",
    "\n",
    "    # Save the processed image\n",
    "    cv2.imwrite(output_path, (img_normalized * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each pair\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    img_path = os.path.join(IMAGE_FOLDER, row['image'])\n",
    "    cloth_path = os.path.join(CLOTH_FOLDER, row['cloth'])\n",
    "\n",
    "    preprocess_image(img_path, os.path.join(PROCESSED_DATA_PATH, \"image\", row['image']), IMG_SIZE)\n",
    "    preprocess_image(cloth_path, os.path.join(PROCESSED_DATA_PATH, \"cloth\", row['cloth']), IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize Processed Images  \n",
    "It's always good to visually check a few processed images to ensure they look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images\n",
    "def plot_images(image_paths, titles, ncols=3):\n",
    "    nrows = int(np.ceil(len(image_paths) / ncols))\n",
    "    plt.figure(figsize=(15, 5 * nrows))\n",
    "    for i, (img_path, title) in enumerate(zip(image_paths, titles)):\n",
    "        img = cv2.imread(img_path)\n",
    "        plt.subplot(nrows, ncols, i + 1)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example visualization\n",
    "sample_images = data.sample(3)\n",
    "image_paths = [os.path.join(PROCESSED_DATA_PATH, \"image\", img) for img in sample_images[\"image\"]]\n",
    "cloth_paths = [os.path.join(PROCESSED_DATA_PATH, \"cloth\", cloth) for cloth in sample_images[\"cloth\"]]\n",
    "plot_images(image_paths + cloth_paths, [\"Image\"] * 3 + [\"Cloth\"] * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experiment with MediaPipe\n",
    "MediaPipe helps analyze pose and other features from images. This step involves loading preprocessed images and running them through MediaPipe for pose detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run MediaPipe on an image\n",
    "def run_mediapipe_on_image(image_path):\n",
    "    \"\"\"\n",
    "    Detect poses using MediaPipe and display key points.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # MediaPipe works with RGB images\n",
    "\n",
    "    # Process the image\n",
    "    results = pose.process(img_rgb)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    annotated_image = img.copy()\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run MediaPipe on one image\n",
    "run_mediapipe_on_image(os.path.join(PROCESSED_DATA_PATH, \"image\", data.iloc[0]['image']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
